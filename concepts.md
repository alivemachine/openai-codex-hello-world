# Computational Generation

It's not clear where the lines are between mathematics, language, computation, data, algorithms, patterns, networks and all the other ways we consider objects and relations, particularly with computers.

What started 100+ years ago as primarily an abstract mathemtical idea - computation - is now present in every aspect of our thinking, science, writing, communicating and society.  There's really no way to take out underlying computational reference to most of what we do.  Even if we deny any ontological value to computation, we mostly plod through modern life using computing to simply get stuff done and communicate.

But with the release of AI that can code the bigger ontological question rears its head.  What exactly is code? what is computation? what do we do when we compute?  

## What is a Computation?

A computation is a map between systems or states of the world. Literally it tells you or a computer how to take one system and get another system.

Usually our concerns as computer users are to create Reliable, Efficient maps.  That is, we wish to have consistent ***outputs*** from consistent ***inputs***.   

Sometimes we wish to do more ***generative*** things where the inputs and outputs don't consistently match, but do maintain some sense of reliable coherence... e.g. I wish to generate random numbers by saying "generate random numbers"... and the system I say to should generate obviously random numbers - the idea of random should be coherent every where it is referenced.

## Is Computation Language? Math? Programming? Algorithms?

Yes.  all of these concepts and activities are facets of a more abstract, general activity of relating to the world systemtically.

It is probably more a relic of history that language and mathematics developed independently.  In programming, computation and now deep learning we find that the boundaries don't exist in theory nor practice.  Modern deep learning systems are very general algorithms that use a ton of data to develop into very advanced probabalistic maps.  The implication being deep learning systems extract lots of algorithms from the data or generate a lot of data in response to data.  

With OpenAI Codex these worlds of language, math, computation all run into each other.  and this is a productive convergence!

## What are you actually suggesting?

OpenAI API Codex does all of these things.  And in a fascinately parallax way.  

If you try to use Codex to write code like you as a human coder do you won't be productive.

If you try to do mathematics or computer algebra with Codex as you do as a human you won't be productive.

If you try to write language things with Codex as you do with your current tools or own brain you won't be productive.

Use CodeX to merge these concepts into a new construct. and new mode of construction.

## The Hyper Computation

A CodeX Hyper Computation is one an activity where you can merge expressive modalities into one superposition object.

A CodeX "session" is one a thinking/creating/searching session by which you can move between thinking, coding, calculating, authoring, poeticizing, visualizing, compiling, debugging, reconfiguring one, in any and all orders you want or see fit.

Consider the main activity of CodeXing as flowing through ideas and branching off insights and interesting ideas as they cohere.  At no point is a CodeX Hyper Computation ever bounded or finalized.

Unlike the programs of our previous computers where it is best for all involved to declare a state of doneness a CodeX app is ever evolving.  

### Why? Why flow?  Why no boundary?

Computers and computer programs that have only one inherent modality cannot remain open.  They require closure to be useful.

A Before-CodeX program could not generate its own input, output and context.  No before program could generate its opearting and value space.

A Before-CodeX program could not generate its own data, own algorithms, own reasoning and use it to feed itself and navigate to new spaces.

Everything is now grist for the mill.

